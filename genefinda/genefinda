#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from re import search, compile, DOTALL, findall
from collections import defaultdict
from difflib import get_close_matches, SequenceMatcher
from argparse import ArgumentParser
import os
from pathlib import Path
from time import time
import logging
from json import loads
from subprocess import Popen, PIPE, run, DEVNULL
from requests import get
import pandas as pd
from genefinda.version import __version__

### Globals ###
bin_path = os.path.dirname(__file__)
db_path = f'{os.path.dirname(bin_path)}/db'
logger = logging.getLogger('root')
logging.basicConfig(format="[%(filename)s | %(funcName)s] %(message)s")
logger.setLevel(logging.DEBUG)


def parse_args():
    parser = ArgumentParser(add_help=False, usage="genefinda reads_1.fq.gz reads_2.fq.gz [options]")
    parser.add_argument('input', nargs='+', type=Path, help='Paired-end fastq(.gz)')
    parser.add_argument('-t', type=int, default='4', help='threads')
    parser.add_argument("-h", action="help", help='show this help message and exit')
    args = parser.parse_args()
    return args


def pubmlst_dict():  # creates a dict for PubMLST Schemes
    d = defaultdict(list)
    fungi = ["afumigatus", "blastocystis", "calbicans", "cglabrata", "ckrusei",
             "ctropicalis", "csinensis", "kseptempunctata", "sparasitica", "tvaginalis"]
    ft = ['fasta', 'csv']
    for line in get('https://pubmlst.org/static/data/dbases.xml').text.split('\n'):
        if search('<url>', line) and not any(f in line for f in fungi) \
                and any(t in line for t in ft):
            key = line.split('pubmlst_')[1].split('_seqdef')[0]
            d[key].append(line.split('>')[1].split('<')[0])
    return d


def pubmlst_map():
    csv = ''
    for m in findall('<species>(.*?)</url>\n</profiles>',
                     get('https://pubmlst.org/static/data/dbases.xml').text, DOTALL):
        scheme = m.split('pubmlst_')[1].split('_seqdef')[0]
        species = m.split(' <mlst>')[0].split('\n<mlst>')[0]
        csv += f'{scheme},{species}\n'
    logger.info(f'writing mapping to {db_path}/mapping.csv')
    with open(f'{db_path}/mapping.csv', 'wt') as f:
        f.write(csv)


def get_ref(path):
    url = 'https://static.onecodex.com/public/finch-rs/refseq_sketches_21_1000.sk.gz'
    # need to gunzip
    with open(path, 'wb') as out:
        logger.info(f'Downloading {url}')
        out.write(get(url).content)
        logger.info(f'Written to {path}')
    return path


def sketch_input(reads, sketch_out):
    cmd = f'cat {reads[0]} {reads[1]} | finch sketch -o {sketch_out} -'
    logger.info(f'{cmd}')
    return run(cmd, shell=True)


def info(sketch):
    cmd = ['finch', 'info', sketch]
    logger.info(f'{" ".join(cmd)}')
    child = Popen(cmd, stdout=PIPE)
    r = child.communicate()[0].decode('utf-8').split('\n')
    kmers = r[1].split(': ')[1]
    depth = r[2].split(': ')[1]
    gc = r[3].split(': ')[1].strip('%')
    logger.info(f'{"{:.1f}".format(int(kmers)/1000000)}Mbp genome, '
                f'{depth} average depth, '
                f'{"{:.1f}".format(float(gc))}% GC')
    return kmers, depth, gc


def dist(sketch, ref):
    cmd = ['finch', 'dist', sketch, ref]
    logger.info(f'{" ".join(cmd)}')
    child = Popen(cmd, stdout=PIPE)
    r = loads(child.communicate()[0])
    return sorted(r, key=lambda k: k['mashDistance'])[:2]

def index(infile, outfile):
    cmd = ['kma', 'index', '-i', '--', '-o', outfile]
    logger.info(f'{" ".join(cmd)}')
    child = Popen(cmd, stdin=PIPE, stderr=DEVNULL)
    child.stdin.write(infile.encode())
    return child.communicate()


def ipe(reads, outfile, percid, scheme, threads):
    cmd = ['kma', '-ipe', reads[0], reads[1], '-ID', percid, #'-1t1',
           '-o', outfile, '-t_db', scheme, '-t', threads]
    # -na -nf -bc90 -mct 0
    logger.info(f'{" ".join(cmd)}')
    child = Popen(cmd, stderr=DEVNULL)
    return child.communicate()


def build_indexes(d):  # builds kma index for each scheme in dictionary
    for k, v in d.items():
        i = ''
        for url in v:
            if 'fasta' in url:
                logger.info(f'indexing {url}')
                i += get(url).text
        index(i, f'{db_path}/indexes/{k}')


def build_tab(d):  # builds kma index for each scheme in dictionary
    for k, v in d.items():
        for url in v:
            if 'csv' in url:
                logger.info(f'downloading {url}')
                with open(f'{db_path}/mapping/{k}.tab', 'wb') as f:
                    f.write(get(url).content)
                logger.info(f'written to {db_path}/mapping/{k}.tab')


def choose_scheme(finch_out):  # chooses best scheme based on finch containment
    d = {}
    with open(f'{db_path}/mapping.csv') as f:
        for line in f:
            d[line.split(',')[1]] = line.split(',')[0]
    match = get_close_matches(finch_out, list(d))[0]
    return d[match]


def main():
    ### Ok, lets go ###
    start = time()
    logger.info(f'genefinda {__version__}')
    logger.info(f'your system is {os.uname()[0]}')
    if 'Linux' not in os.uname()[0]:
        logger.warning(f'genefinda has not been tested on {os.uname()[0]}')

    #formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')

    logger.info(f'cwd is {os.getcwd()}')
    #logger.info(f'bin path is {bin_path}')
    #logger.info(f'db path is {db_path}')
    args = parse_args()

    ### Inbuilt scheme index builder ###

    if f'{str(args.input[0])} {str(args.input[1])}' == 'update indexes':
        logger.info(f'building scheme fasta indexes')
        build_indexes(pubmlst_dict())
        exit(logger.info(f'completed in {"{:.1f}".format(time() - start)} seconds'))

    elif f'{str(args.input[0])} {str(args.input[1])}' == 'update schemes':
        logger.info(f'downloading scheme csv files')
        build_tab(pubmlst_dict())
        exit(logger.info(f'completed in {"{:.1f}".format(time() - start)} seconds'))

    elif f'{str(args.input[0])} {str(args.input[1])}' == 'update map':
        logger.info(f'creating scheme to species map')
        pubmlst_map()
        exit(logger.info(f'completed in {"{:.1f}".format(time() - start)} seconds'))

    ### Run main program ###
    else:
        ### Sanity Checks ###

        threads = str(args.t)
        if args.t > os.cpu_count():
            logger.warning(f'number of threads exceeds available CPUs')
            threads = str(os.cpu_count())
        logger.info(f'using {threads} threads')

        ref = f'{db_path}/refseq_sketches_21_1000.sk'
        if not os.path.isfile(ref):
            logger.warning(f'{ref} not found')
            get_ref(ref)

        ### Check input filetypes and create read-pair dict ###
        suffixes = ['_R[12]\.(fastq(?:\.gz)?)$', '_R[12]_[0-9]+?\.(fastq(?:\.gz)?)$',
                    '_[12]\.(fastq(?:\.gz)?)$', '_R[12]\.(fq(?:\.gz)?)$',
                    '_R[12]_[0-9]+?\.(fq(?:\.gz)?)$', '_[12]\.(fq(?:\.gz)?)$']

        r = compile('|'.join(suffixes))
        pairs = defaultdict(list)
        for i in args.input:
            if not os.path.isfile(i):
                logger.error(f'{i} is not a valid file')
            else:
                filename = str(i)
                s = search(r, filename)
                if s:
                    pairs[filename.replace(s.group(0), '')].append(str(i))
                else:
                    logger.error(f'{i} is not a fastq file')

        ### Loop over read-pair dict ###
        for name in pairs.keys():
            sample = os.path.basename(name)
            logger.info(f'read pair for {sample}: {" ".join(pairs[name])}')
            sketch = f'{name}.sk'
            if not os.path.isfile(sketch):
                logger.info(f'read pair for {sample}: {" ".join(pairs[name])}')
                sketch_input(pairs[name], sketch)
            info(sketch)
            finch = dist(sketch, ref)
            s1 = finch[0]["reference"]
            logger.info(f'closest refseq genome is {s1}')
            # s2 = finch[1]["reference"]
            # logger.info(f'second-closest refseq genome is {s2}')
            scheme = choose_scheme(s1)
            logger.info(f'choosing scheme: {scheme}')
            ipe(pairs[name], f'{name}', '90', f'{db_path}/indexes/{scheme}', threads)
            df = pd.read_csv(f'{name}.res', delimiter='\t')
            df[['#Template', 'allele']] = df['#Template'].str.split('_', expand=True)
            df = df[df.groupby(['#Template'])['q_value'].transform(max) == df['q_value']]
            df = df.set_index('#Template').T.astype(int)
            df1 = pd.read_csv(f'{db_path}/mapping/{scheme}.tab', delimiter='\t', index_col=False)
            mlst = df1.merge(df, on=list(df.columns.intersection(df1.columns)), how='inner')
            print(mlst.to_csv(sep='\t', index=False))

    logger.info(f'completed in {"{:.1f}".format(time() - start)} seconds')

if __name__ == '__main__':
    main()
